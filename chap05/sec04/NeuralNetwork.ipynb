{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_neurons,  # 入力層のニューロン数\n",
    "        hidden_neurons, # 隠れ層のニューロン数\n",
    "        output_neurons, # 出力層のニューロン数\n",
    "        learning_rate   # 学習率\n",
    "        ):\n",
    "        '''\n",
    "        ニューラルネットワークの初期化を行う\n",
    "\n",
    "        '''\n",
    "        # 入力層、隠れ層、出力層のニューロン数をインスタンス変数に代入\n",
    "        self.inneurons = input_neurons # 入力層のニューロン数\n",
    "        self.hneurons = hidden_neurons # 隠れ層のニューロン数\n",
    "        self.oneurons = output_neurons # 出力層のニューロン数\n",
    "        self.lr = learning_rate        # 学習率\n",
    "        self.weight_initializer()      # weight_initializer()を呼ぶ\n",
    "\n",
    "    def weight_initializer(self):\n",
    "        '''\n",
    "        重みとバイアスの初期化を行う\n",
    "        \n",
    "        '''\n",
    "        # 隠れ層の重みとバイアスを初期化\n",
    "        self.w1 = np.random.normal(\n",
    "            0.0,                       # 平均は0\n",
    "            pow(self.inneurons, -0.5), # 標準偏差は入力層のニューロン数を元に計算\n",
    "            (self.hneurons,            # 行数は隠れ層のニューロン数\n",
    "             self.inneurons + 1)       # 列数は入力層のニューロン数 + 1\n",
    "            )\n",
    "        \n",
    "       # 出力層の重みとバイアスを初期化\n",
    "        self.w2 = np.random.normal(\n",
    "            0.0,                      # 平均は0\n",
    "            pow(self.hneurons, -0.5), # 標準偏差は隠れ層のニューロン数を元に計算\n",
    "            (self.oneurons,           # 行数は出力層のニューロン数\n",
    "             self.hneurons + 1)       # 列数は隠れ層のニューロン数 + 1\n",
    "            )\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        シグモイド関数\n",
    "        ------------------------\n",
    "        x : 関数を適用するデータ\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "        ソフトマックス関数\n",
    "        ------------------------\n",
    "        x : 関数を適用するデータ\n",
    "        '''\n",
    "        c = np.max(x)\n",
    "        exp_x = np.exp(x - c) # オーバーフローを防止する\n",
    "        sum_exp_x = np.sum(exp_x)\n",
    "        y = exp_x / sum_exp_x\n",
    "        return y\n",
    "    \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        '''\n",
    "        ニューラルネットワークの学習を行う\n",
    "        ------------------------\n",
    "        inputs_list  : 訓練データの配列\n",
    "        targets_list : 正解ラベルの配列\n",
    "        \n",
    "        '''\n",
    "        ## [入力層]\n",
    "        # 入力値の配列にバイアス項を追加して入力層から出力する\n",
    "        inputs = np.array(\n",
    "            np.append(\n",
    "                inputs_list, [1]), # 配列の末尾にバイアスのための「1」を追加\n",
    "            ndmin=2                # 2次元化\n",
    "        ).T                        # 転置して1列の行列にする\n",
    "\n",
    "        ## [隠れ層]\n",
    "        # 入力層の出力に重み、バイアスを適用して隠れ層に入力する\n",
    "        hidden_inputs = np.dot(\n",
    "            self.w1,              # 隠れ層の重み\n",
    "            inputs                # 入力層の出力\n",
    "            )\n",
    "        # シグモイド関数を適用して隠れ層から出力\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)        \n",
    "        # 隠れ層の出力行列の末尾にバイアスのための「1」を追加\n",
    "        hidden_outputs = np.append(\n",
    "            hidden_outputs,      # 隠れ層の出力行列\n",
    "            [[1]],               # 2次元形式でバイアス値を追加\n",
    "            axis=0               # 行を指定(列は1)\n",
    "            )\n",
    "        \n",
    "        ## [出力層]\n",
    "        # 出力層への入力信号を計算\n",
    "        final_inputs = np.dot(\n",
    "            self.w2,             # 隠れ層と出力層の間の重み\n",
    "            hidden_outputs       # 隠れ層の出力\n",
    "            )\n",
    "        # ソフトマックス関数を適用して出力層から出力する\n",
    "        final_outputs = self.softmax(final_inputs)\n",
    "        \n",
    "        ## ---バックプロパゲーション---(出力層)\n",
    "        # 正解ラベルの配列を1列の行列に変換する\n",
    "        targets = np.array(\n",
    "            targets_list,        # 正解ラベルの配列\n",
    "            ndmin=2              # 2次元化\n",
    "            ).T                  # 転置して1列の行列にする\n",
    "        # 出力値と正解ラベルとの誤差\n",
    "        output_errors = final_outputs - targets\n",
    "        # 出力層の入力誤差δを求める\n",
    "        delta_output = output_errors*(1 - final_outputs)*final_outputs\n",
    "        # 重みを更新する前に隠れ層の出力誤差を求めておく\n",
    "        hidden_errors = np.dot(\n",
    "            self.w2.T,           # 出力層の重み行列を転置する\n",
    "            delta_output         # 出力層の入力誤差δ\n",
    "            )\n",
    "        # 出力層の重み、バイアスの更新\n",
    "        self.w2 -= self.lr * np.dot(\n",
    "            # 出力誤差＊(1－出力信号)＊出力信号 \n",
    "            delta_output,\n",
    "            # 隠れ層の出力行列を転置\n",
    "            hidden_outputs.T\n",
    "        )\n",
    "        \n",
    "        ## ---バックプロパゲーション---(隠れ層)\n",
    "        # 逆伝搬された隠れ層の出力誤差からバイアスのものを取り除く\n",
    "        hidden_errors_nobias = np.delete(\n",
    "            hidden_errors,      # 隠れ層のエラーの行列\n",
    "            self.hneurons,      # 隠れ層のニューロン数をインデックスにする\n",
    "            axis=0              # 行の削除を指定\n",
    "            )\n",
    "        # 隠れ層の出力行列からバイアスを除く\n",
    "        hidden_outputs_nobias = np.delete(\n",
    "            hidden_outputs,     # 隠れ層の出力の行列\n",
    "            self.hneurons,      # 隠れ層のニューロン数をインデックスにする\n",
    "            axis=0              # 行の削除を指定\n",
    "            )\n",
    "        # 隠れ層の重み、バイアスの更新\n",
    "        self.w1 -= self.lr * np.dot(\n",
    "            # 逆伝搬された隠れ層の出力誤差＊(1－隠れ層の出力)＊隠れ層の出力 \n",
    "            hidden_errors_nobias*(\n",
    "                1.0 - hidden_outputs_nobias\n",
    "            )*hidden_outputs_nobias,\n",
    "            # 入力層の出力信号の行列を転置\n",
    "            inputs.T\n",
    "            )\n",
    "   \n",
    "    def evaluate(self,\n",
    "                 inputs_list\n",
    "                ):\n",
    "        '''\n",
    "        学習した重みでテストデータを評価する\n",
    "        ------------------------------------\n",
    "        inputs_list : テスト用データの配列\n",
    "        \n",
    "        '''\n",
    "        ## [入力層]\n",
    "        # 入力値の配列にバイアス項を追加して入力層から出力する\n",
    "        inputs = np.array(\n",
    "            np.append(inputs_list, [1]), # 配列の末尾にバイアスの値「1」を追加\n",
    "            ndmin=2                      # 2次元化\n",
    "        ).T                              # 転置して1列の行列にする\n",
    "        \n",
    "        ## [隠れ層]\n",
    "        # 入力層の出力に重み、バイアスを適用して隠れ層に入力する\n",
    "        hidden_inputs = np.dot(self.w1,  # 入力層と隠れ層の間の重み\n",
    "                               inputs    # テストデータの行列\n",
    "                              )       \n",
    "        # 活性化関数を適用して隠れ層から出力する\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
    "        \n",
    "        ## [出力層]\n",
    "        # 出力層への入力信号を計算\n",
    "        final_inputs = np.dot(\n",
    "            self.w2,                        # 隠れ層と出力層の間の重み\n",
    "            np.append(hidden_outputs, [1]), # 隠れ層の出力配列の末尾にバイアスの値「1」を追加\n",
    "            )       \n",
    "        # 活性化関数を適用して出力層から出力する\n",
    "        final_outputs = self.softmax(final_inputs)\n",
    "        \n",
    "        # 出力層からの出力を戻り値として返す\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
