{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータの準備\n",
    "import os\n",
    "import json\n",
    "\n",
    "# コーパスのディレクトリを設定\n",
    "file_path = './projectnextnlp-chat-dialogue-corpus/json/rest1046/'\n",
    "# ファイルの一覧を取得\n",
    "file_dir = os.listdir(file_path)\n",
    "# 正解ラベルとテキストデータを保持するリスト\n",
    "label_text = []\n",
    "\n",
    "# ファイルごとに対話データを整形する\n",
    "for file in file_dir:\n",
    "    # JSONファイルの読み込み\n",
    "    r = open(file_path + file, 'r', encoding='utf-8')\n",
    "    json_data = json.load(r)\n",
    "        \n",
    "    # 発話データ配列から発話テキストと破綻かどうかの正解データを抽出\n",
    "    for turn in json_data['turns']:\n",
    "        turn_index = turn['turn-index'] # turn-indexキー(対話のインデックス)\n",
    "        speaker = turn['speaker']       # speakerキー(\"U\"人間、\"S\"システム)\n",
    "        utterance = turn['utterance']   # utteranceキー(発話テキスト)\n",
    "        \n",
    "        # 先頭行(システムの冒頭の発話)以外を処理\n",
    "        if turn_index != 0:\n",
    "            # 人間の発話（質問）のテキストを抽出\n",
    "            if speaker == 'U':\n",
    "                #u_text = ''\n",
    "                u_text = utterance\n",
    "\n",
    "            # システムの回答内容が破綻かどうかを抽出\n",
    "            else:\n",
    "                a = ''\n",
    "                sys = turn['utterance'] # システムの発話（応答）を抽出\n",
    "                t = turn['annotations'][0] # １つ目のアノテーションを抽出                \n",
    "                a = t['breakdown']      # アノテーションのフラグを抽出\n",
    "                if a == 'O':            # O（破綻していない）を0で置換\n",
    "                    label = 0\n",
    "                else:                   # O以外は破綻とし1で置換\n",
    "                    label = 1\n",
    "                # 正解ラベルをタブで区切って「人間の発話#システムの発話」を連結\n",
    "                tmp1 = str(label) + '\\t' + u_text + ' # ' +sys\n",
    "                # タブで区切ってリストにする\n",
    "                tmp2 = tmp1.split('\\t')\n",
    "                # 作成したリストを要素としてlabel_textに追加\n",
    "                label_text.append(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素への分解と正解ラベルのリストの生成\n",
    "from janome.tokenizer import Tokenizer # janomeのパッケージをインポート\n",
    "import re                              # 正規表現ライブラリ\n",
    "\n",
    "t = Tokenizer()                       # Tokenizerクラスのオブジェクトを生成\n",
    "separation_tmp = []                   # 形態素を一時保存するリスト\n",
    "\n",
    "# 形態素に分解\n",
    "for row in label_text:\n",
    "    # リストから発話テキストの部分を抽出して形態素解析を実行\n",
    "    tokens = t.tokenize(row[1])\n",
    "    # 形態素の見出しの部分を取得してseparation_tmpに追加\n",
    "    separation_tmp.append(\n",
    "        [token.surface for token in tokens if (\n",
    "            not re.match('記号', token.part_of_speech)             # 記号を除外\n",
    "            and (not re.match('助詞', token.part_of_speech))       # 助詞を除外\n",
    "            and (not re.match('助動詞', token.part_of_speech))     # 助動詞を除外\n",
    "            and (not re.match('動詞,非自立', token.part_of_speech))# 動詞,非自立を除外\n",
    "            and (not re.match('名詞,非自立', token.part_of_speech))# 名詞,非自立を除外\n",
    "            )\n",
    "         ])\n",
    "    # 空の要素があれば取り除く\n",
    "    while separation_tmp.count('') > 0:\n",
    "        separation_tmp.remove('')\n",
    "\n",
    "# 正解ラベルをint型に変換してリストに格納\n",
    "train_y_tmp = [int(label[0]) for label in label_text]\n",
    "\n",
    "separation = [] # 形態素のリスト\n",
    "train_y = []    # 正解ラベルのリスト\n",
    "\n",
    "# separation_tmpの値が存在すれば、\n",
    "# 形態素のリストと正解ラベルのリストを作成\n",
    "for x, y in zip(separation_tmp, train_y_tmp):\n",
    "    if x:\n",
    "        separation.append(x)\n",
    "        train_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の出現回数を記録して辞書を作成　\n",
    "from collections import Counter        # カウント処理のためのライブラリ\n",
    "import itertools                       # イテレーションのためのライブラリ\n",
    "# {単語：出現回数}の辞書を作成\n",
    "word_frequency = Counter(itertools.chain(* separation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語を頻度降順に並べ替え\n",
    "word_list = []\n",
    "# most_common()で出現回数順に要素を取得しword_listに追加\n",
    "for w in word_frequency.most_common():\n",
    "    word_list.append(w[0])\n",
    "    \n",
    "# 頻度順に並べた単語をキーに、1から始まる連番を値に設定\n",
    "word_dic = {}\n",
    "for i, word in enumerate(word_list, start=1):\n",
    "    word_dic.update({word: i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語を出現頻度の数値に置き替える\n",
    "train_x = [[ word_dic[word] for word in sp] for sp in separation ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlowライブラリ\n",
    "import tensorflow as tf\n",
    "# TFLearnライブラリ\n",
    "import tflearn\n",
    "# データの前処理を行うライブラリ\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# 単語データの配列のサイズを揃える\n",
    "trainX = pad_sequences(train_x, maxlen=32, value=0.)\n",
    "# # 正解ラベルをOne-hot表現にする\n",
    "trainY = to_categorical(train_y, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Toshiya\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# RNNの構築\n",
    "\n",
    "# 初期化\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## 入力層の作成\n",
    "net = tflearn.input_data([None, 32])\n",
    "\n",
    "## 中間層の作成\n",
    "# 単語埋め込み層\n",
    "net = tflearn.embedding(\n",
    "    net,\n",
    "    input_dim=len(word_dic) + 1, # 単語の総数に0のための1を加算,\n",
    "    output_dim=128\n",
    ")\n",
    "\n",
    "# LSTMブロック\n",
    "net = tflearn.lstm(net, 128, dropout=0.5, return_seq=True)\n",
    "net = tflearn.lstm(net, 128, dropout=0.5, return_seq=True)\n",
    "net = tflearn.lstm(net, 128, dropout=0.5, return_seq=False)\n",
    "\n",
    "## 出力層の作成 \n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "\n",
    "# 学習条件の設定\n",
    "net = tflearn.regression(net,\n",
    "                         optimizer='adam',\n",
    "                         learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 13099  | total loss: \u001b[1m\u001b[32m0.64341\u001b[0m\u001b[0m | time: 23.016s\n",
      "| Adam | epoch: 050 | loss: 0.64341 - acc: 0.6676 -- iter: 8352/8368\n",
      "Training Step: 13100  | total loss: \u001b[1m\u001b[32m0.64006\u001b[0m\u001b[0m | time: 24.511s\n",
      "| Adam | epoch: 050 | loss: 0.64006 - acc: 0.6727 | val_loss: 0.69171 - val_acc: 0.5875 -- iter: 8368/8368\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(trainX,\n",
    "          trainY,\n",
    "          n_epoch=50,\n",
    "          batch_size=32,\n",
    "          validation_set=0.2,\n",
    "          shuffle=True,\n",
    "          show_metric=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
